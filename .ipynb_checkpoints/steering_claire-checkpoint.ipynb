{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vWjp0YakvnjK"
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch.nn.utils.rnn import *\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "import time\n",
    "import pdb\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1_FQdqcCbdf_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPU = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if GPU else \"cpu\")\n",
    "cuda = torch.cuda.is_available()\n",
    "num_workers = 0 if cuda else 0\n",
    "GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading complete, took 4 seconds\n",
      "# Images loaded:  300\n"
     ]
    }
   ],
   "source": [
    "numDataPointsWanted = 300\n",
    "X = np.zeros((numDataPointsWanted, 3, 480, 640))\n",
    "Y = np.zeros((numDataPointsWanted, 3))\n",
    "dataType = \"center\"\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "root_path = \"./\"\n",
    "with open(root_path + \"labels.txt\") as f:\n",
    "    start = time.time()\n",
    "    i = 0\n",
    "    for line in f:\n",
    "        [filename, radians, torque, speed] = line.split(' ')\n",
    "\n",
    "        if filename[0:6] == \"center\":\n",
    "            with Image.open(root_path + filename) as img:\n",
    "                X[i,:,:,:] = (transform(img.convert('RGB')))\n",
    "            Y[i,0] = (((float(radians) * 180.0)/np.pi))\n",
    "            Y[i,1] = float(torque)\n",
    "            Y[i,2] = float(speed)\n",
    "            i += 1\n",
    "            \n",
    "        if i == numDataPointsWanted: break\n",
    "    end = time.time()\n",
    "    \n",
    "\n",
    "print(\"Data loading complete, took\", int(end - start), \"seconds\")\n",
    "print('# Images loaded: ', len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, valX, trainY, valY = train_test_split(X, Y, test_size=0.30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E-AvSJUVAdby"
   },
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, X, Y, seq_len):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X) // self.seq_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        idx = self.seq_len * index\n",
    "        return self.X[idx:idx + self.seq_len], self.Y[idx + self.seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "num_workers = 0 if cuda else 0\n",
    "\n",
    "train_set = Dataset(trainX, trainY, 15)\n",
    "train_dataloader = data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers)\n",
    "val_set = Dataset(valX, valY, 15)\n",
    "val_dataloader = data.DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        assert hidden_channels % 2 == 0\n",
    "\n",
    "        self.input_channels = input_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_features = 4\n",
    "\n",
    "        self.padding = int((kernel_size - 1) / 2)\n",
    "\n",
    "        self.Wxi = nn.Conv2d(self.input_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=True)\n",
    "        self.Whi = nn.Conv2d(self.hidden_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=False)\n",
    "        self.Wxf = nn.Conv2d(self.input_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=True)\n",
    "        self.Whf = nn.Conv2d(self.hidden_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=False)\n",
    "        self.Wxc = nn.Conv2d(self.input_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=True)\n",
    "        self.Whc = nn.Conv2d(self.hidden_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=False)\n",
    "        self.Wxo = nn.Conv2d(self.input_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=True)\n",
    "        self.Who = nn.Conv2d(self.hidden_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=False)\n",
    "\n",
    "        self.Wci = None\n",
    "        self.Wcf = None\n",
    "        self.Wco = None\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        ci = torch.sigmoid(self.Wxi(x) + self.Whi(h) + c * self.Wci)\n",
    "        cf = torch.sigmoid(self.Wxf(x) + self.Whf(h) + c * self.Wcf)\n",
    "        cc = cf * c + ci * torch.tanh(self.Wxc(x) + self.Whc(h))\n",
    "        co = torch.sigmoid(self.Wxo(x) + self.Who(h) + cc * self.Wco)\n",
    "        ch = co * torch.tanh(cc)\n",
    "        return ch, cc\n",
    "\n",
    "    def init_hidden(self, batch_size, hidden, shape):\n",
    "        if self.Wci is None:\n",
    "            self.Wci = Variable(torch.zeros(1, hidden, shape[0], shape[1])).to(device)\n",
    "            self.Wcf = Variable(torch.zeros(1, hidden, shape[0], shape[1])).to(device)\n",
    "            self.Wco = Variable(torch.zeros(1, hidden, shape[0], shape[1])).to(device)\n",
    "        else:\n",
    "            assert shape[0] == self.Wci.size()[2], 'Input Height Mismatched!'\n",
    "            assert shape[1] == self.Wci.size()[3], 'Input Width Mismatched!'\n",
    "        return (Variable(torch.zeros(batch_size, hidden, shape[0], shape[1])).to(device),\n",
    "                Variable(torch.zeros(batch_size, hidden, shape[0], shape[1])).to(device))\n",
    "\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    # input_channels corresponds to the first input feature map\n",
    "    # hidden state is a list of succeeding lstm layers.\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size, step=1, effective_step=[1]):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        self.input_channels = [input_channels] + hidden_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = len(hidden_channels)\n",
    "        self.step = step\n",
    "        self.effective_step = effective_step\n",
    "        self._all_layers = []\n",
    "        for i in range(self.num_layers):\n",
    "            name = 'cell{}'.format(i)\n",
    "            cell = ConvLSTMCell(self.input_channels[i], self.hidden_channels[i], self.kernel_size)\n",
    "            setattr(self, name, cell)\n",
    "            self._all_layers.append(cell)\n",
    "\n",
    "    def forward(self, input):\n",
    "        internal_state = []\n",
    "        outputs = []\n",
    "        for step in range(self.step):\n",
    "            x = input\n",
    "            for i in range(self.num_layers):\n",
    "                # all cells are initialized in the first step\n",
    "                name = 'cell{}'.format(i)\n",
    "                if step == 0:\n",
    "                    bsize, _, height, width = x.size()\n",
    "                    (h, c) = getattr(self, name).init_hidden(batch_size=bsize, hidden=self.hidden_channels[i],\n",
    "                                                             shape=(height, width))\n",
    "                    internal_state.append((h, c))\n",
    "\n",
    "                # do forward\n",
    "                (h, c) = internal_state[i]\n",
    "                x, new_c = getattr(self, name)(x, h, c)\n",
    "                internal_state[i] = (x, new_c)\n",
    "            # only record effective steps\n",
    "            outputs.append(x)\n",
    "\n",
    "        return outputs, (x, new_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Whip(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, input_size, output_size):\n",
    "        super(Whip, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=(1, 4, 4), stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=(3, 12, 12), stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.conv3 = nn.Conv3d(out_channels, out_channels, kernel_size=(1, 4, 4), stride=2)\n",
    "        self.bn3 = nn.BatchNorm3d(out_channels)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        self.avgpool = nn.AvgPool3d(1)\n",
    "        \n",
    "        self.convLSTM = ConvLSTM(input_channels=out_channels, hidden_channels= [512, 128, 64, 32], \n",
    "                                 kernel_size=3)\n",
    "        \n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.conv1(x) \n",
    "        output = self.bn1(output)\n",
    "        output = self.relu1(output)\n",
    "        \n",
    "        output = self.conv2(output)\n",
    "        output = self.bn2(output)\n",
    "        output = self.relu2(output)\n",
    "        \n",
    "        output = self.conv3(output)\n",
    "        output = self.bn3(output)\n",
    "        output = self.relu3(output)\n",
    "        \n",
    "        output = self.avgpool(output).squeeze(2)\n",
    "        \n",
    "        output, hidden = self.convLSTM(output)\n",
    "        output = output[0]\n",
    "#         pdb.set_trace()\n",
    "        \n",
    "        featuresV = output.reshape(output.size(0), -1)\n",
    "        output = self.fc(featuresV)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whip(\n",
      "  (conv1): Conv3d(15, 13, kernel_size=(1, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "  (bn1): BatchNorm3d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu1): ReLU()\n",
      "  (conv2): Conv3d(13, 13, kernel_size=(3, 12, 12), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "  (bn2): BatchNorm3d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu2): ReLU()\n",
      "  (conv3): Conv3d(13, 13, kernel_size=(1, 4, 4), stride=(2, 2, 2))\n",
      "  (bn3): BatchNorm3d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu3): ReLU()\n",
      "  (avgpool): AvgPool3d(kernel_size=1, stride=1, padding=0)\n",
      "  (convLSTM): ConvLSTM(\n",
      "    (cell0): ConvLSTMCell(\n",
      "      (Wxi): Conv2d(13, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (Whi): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (Wxf): Conv2d(13, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (Whf): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (Wxc): Conv2d(13, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (Whc): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (Wxo): Conv2d(13, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (Who): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (cell1): ConvLSTMCell(\n",
      "      (Wxi): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (Whi): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (Wxf): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (Whf): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (Wxc): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (Whc): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (Wxo): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (Who): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (cell2): ConvLSTMCell(\n",
      "      (Wxi): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (Whi): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (Wxf): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (Whf): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (Wxc): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (Whc): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (Wxo): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (Who): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (cell3): ConvLSTMCell(\n",
      "      (Wxi): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (Whi): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (Wxf): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (Whf): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (Wxc): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (Whc): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (Wxo): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (Who): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=46800, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "context_size = 15\n",
    "out_channels = 13\n",
    "input_size = 46800\n",
    "\n",
    "# we will be predicting angle, torque, and speed\n",
    "output_size = 3\n",
    "model = Whip(context_size, out_channels, input_size, output_size).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=1)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-113-19f916fbeae8>(42)forward()\n",
      "-> featuresV = output.reshape(output.size(0), -1)\n",
      "(Pdb) output\n",
      "[tensor([[[[-1.1149e-05, -3.7266e-04, -4.0491e-04,  ..., -6.7988e-04,\n",
      "           -6.1771e-04, -2.9105e-04],\n",
      "          [ 3.1029e-04, -3.4854e-04, -4.2609e-04,  ..., -5.3043e-04,\n",
      "           -4.6984e-04, -5.4282e-04],\n",
      "          [ 2.1471e-04, -3.0204e-04, -4.9812e-04,  ..., -4.1323e-04,\n",
      "           -3.8456e-04, -5.2362e-04],\n",
      "          ...,\n",
      "          [ 2.2883e-04, -3.8108e-04, -6.3531e-04,  ..., -4.9166e-06,\n",
      "           -5.5533e-04, -8.6345e-04],\n",
      "          [ 3.1564e-04, -2.7723e-05, -1.2721e-04,  ..., -2.1178e-05,\n",
      "            1.5566e-05, -5.5196e-04],\n",
      "          [ 3.3612e-04, -9.4002e-06, -3.9635e-05,  ..., -1.5769e-04,\n",
      "           -1.9560e-04, -2.0855e-04]],\n",
      "\n",
      "         [[-1.0658e-02, -1.1162e-02, -1.1068e-02,  ..., -1.0986e-02,\n",
      "           -1.1052e-02, -1.1347e-02],\n",
      "          [-1.0339e-02, -1.0768e-02, -1.0824e-02,  ..., -1.0728e-02,\n",
      "           -1.0734e-02, -1.1315e-02],\n",
      "          [-1.0339e-02, -1.0734e-02, -1.0679e-02,  ..., -1.0918e-02,\n",
      "           -1.0873e-02, -1.1433e-02],\n",
      "          ...,\n",
      "          [-1.0573e-02, -1.1363e-02, -1.0851e-02,  ..., -1.1594e-02,\n",
      "           -1.1257e-02, -1.1777e-02],\n",
      "          [-1.0717e-02, -1.1511e-02, -1.1392e-02,  ..., -1.1496e-02,\n",
      "           -1.1419e-02, -1.1735e-02],\n",
      "          [-1.0114e-02, -1.0760e-02, -1.0634e-02,  ..., -1.0393e-02,\n",
      "           -1.0824e-02, -1.1470e-02]],\n",
      "\n",
      "         [[ 7.6292e-03,  7.6521e-03,  7.5211e-03,  ...,  7.7183e-03,\n",
      "            7.7304e-03,  7.3119e-03],\n",
      "          [ 8.0086e-03,  8.4498e-03,  8.4653e-03,  ...,  8.9027e-03,\n",
      "            8.7931e-03,  8.1507e-03],\n",
      "          [ 8.0114e-03,  8.4757e-03,  8.4965e-03,  ...,  8.8546e-03,\n",
      "            8.6629e-03,  8.1015e-03],\n",
      "          ...,\n",
      "          [ 8.3707e-03,  8.9133e-03,  8.7611e-03,  ...,  8.5043e-03,\n",
      "            8.5561e-03,  8.1042e-03],\n",
      "          [ 8.2450e-03,  8.8575e-03,  8.6626e-03,  ...,  8.8745e-03,\n",
      "            8.6903e-03,  8.1051e-03],\n",
      "          [ 7.9238e-03,  8.3990e-03,  8.4325e-03,  ...,  8.7396e-03,\n",
      "            8.3814e-03,  8.0915e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.8890e-03, -2.8394e-03, -2.8950e-03,  ..., -2.8384e-03,\n",
      "           -2.8372e-03, -2.8781e-03],\n",
      "          [-3.0539e-03, -3.0581e-03, -2.9318e-03,  ..., -3.0147e-03,\n",
      "           -2.9929e-03, -3.0445e-03],\n",
      "          [-2.9386e-03, -3.0239e-03, -2.9927e-03,  ..., -3.0744e-03,\n",
      "           -3.0821e-03, -2.9902e-03],\n",
      "          ...,\n",
      "          [-2.9274e-03, -2.8429e-03, -2.6505e-03,  ..., -2.6401e-03,\n",
      "           -2.6036e-03, -2.7604e-03],\n",
      "          [-2.9371e-03, -3.0162e-03, -2.7060e-03,  ..., -2.8324e-03,\n",
      "           -2.7641e-03, -2.7434e-03],\n",
      "          [-3.4203e-03, -3.2962e-03, -3.3023e-03,  ..., -3.0148e-03,\n",
      "           -3.3834e-03, -3.0546e-03]],\n",
      "\n",
      "         [[ 1.2001e-03,  1.1750e-03,  1.1427e-03,  ...,  1.1606e-03,\n",
      "            1.0324e-03,  9.3777e-04],\n",
      "          [ 9.0607e-04,  8.4450e-04,  9.5732e-04,  ...,  7.7797e-04,\n",
      "            7.2421e-04,  8.8872e-04],\n",
      "          [ 9.5271e-04,  9.2466e-04,  9.5184e-04,  ...,  8.8689e-04,\n",
      "            8.3424e-04,  9.3114e-04],\n",
      "          ...,\n",
      "          [ 1.0097e-03,  1.0904e-03,  8.0937e-04,  ...,  1.2071e-03,\n",
      "            7.6083e-04,  1.0782e-03],\n",
      "          [ 8.2091e-04,  7.1688e-04,  7.3349e-04,  ...,  1.1200e-03,\n",
      "            8.2861e-04,  1.0291e-03],\n",
      "          [ 1.2663e-03,  1.2364e-03,  9.8389e-04,  ...,  1.0081e-03,\n",
      "            1.2356e-03,  1.0217e-03]],\n",
      "\n",
      "         [[ 1.9756e-03,  2.3598e-03,  2.3985e-03,  ...,  2.3953e-03,\n",
      "            2.4453e-03,  2.4411e-03],\n",
      "          [ 1.8747e-03,  1.8638e-03,  1.8973e-03,  ...,  1.7965e-03,\n",
      "            1.7082e-03,  1.7874e-03],\n",
      "          [ 1.8926e-03,  1.8490e-03,  1.8498e-03,  ...,  1.8515e-03,\n",
      "            1.7839e-03,  1.8711e-03],\n",
      "          ...,\n",
      "          [ 1.7033e-03,  1.4808e-03,  1.8291e-03,  ...,  1.9435e-03,\n",
      "            1.9645e-03,  2.4724e-03],\n",
      "          [ 1.7218e-03,  1.5754e-03,  1.9831e-03,  ...,  1.7133e-03,\n",
      "            1.4185e-03,  2.0268e-03],\n",
      "          [ 1.9756e-03,  1.5634e-03,  1.7487e-03,  ...,  1.7492e-03,\n",
      "            1.6020e-03,  1.7844e-03]]],\n",
      "\n",
      "\n",
      "        [[[-2.2856e-05, -3.4150e-04, -4.4304e-04,  ..., -3.5844e-04,\n",
      "           -4.6138e-04, -2.2324e-04],\n",
      "          [ 3.2256e-04, -3.8031e-04, -5.0165e-04,  ..., -1.7511e-04,\n",
      "           -3.2802e-04, -5.2747e-04],\n",
      "          [ 2.0438e-04, -3.2561e-04, -4.3038e-04,  ..., -3.7582e-04,\n",
      "           -4.5592e-04, -6.1062e-04],\n",
      "          ...,\n",
      "          [ 2.8154e-04, -4.0708e-04, -4.1935e-04,  ..., -1.7898e-04,\n",
      "           -5.5578e-04, -6.4834e-04],\n",
      "          [ 2.2283e-04, -3.8432e-04, -4.2754e-04,  ..., -3.3883e-04,\n",
      "           -3.8177e-04, -5.2773e-04],\n",
      "          [ 2.2452e-04, -1.8469e-04, -2.5578e-04,  ..., -2.5621e-04,\n",
      "           -3.2012e-04, -5.0414e-04]],\n",
      "\n",
      "         [[-1.0734e-02, -1.1198e-02, -1.1178e-02,  ..., -1.1042e-02,\n",
      "           -1.1028e-02, -1.1241e-02],\n",
      "          [-1.0379e-02, -1.0735e-02, -1.0611e-02,  ..., -1.0806e-02,\n",
      "           -1.0684e-02, -1.1175e-02],\n",
      "          [-1.0351e-02, -1.0735e-02, -1.0771e-02,  ..., -1.0952e-02,\n",
      "           -1.0822e-02, -1.1385e-02],\n",
      "          ...,\n",
      "          [-1.0333e-02, -1.0689e-02, -1.0781e-02,  ..., -1.0814e-02,\n",
      "           -1.0758e-02, -1.1317e-02],\n",
      "          [-1.0378e-02, -1.0798e-02, -1.0764e-02,  ..., -1.0869e-02,\n",
      "           -1.0757e-02, -1.1326e-02],\n",
      "          [-1.0044e-02, -1.0419e-02, -1.0361e-02,  ..., -1.0302e-02,\n",
      "           -1.0513e-02, -1.1053e-02]],\n",
      "\n",
      "         [[ 7.6481e-03,  7.6458e-03,  7.5017e-03,  ...,  7.7823e-03,\n",
      "            7.7709e-03,  7.2785e-03],\n",
      "          [ 8.0801e-03,  8.5751e-03,  8.4888e-03,  ...,  8.7852e-03,\n",
      "            8.6503e-03,  8.0706e-03],\n",
      "          [ 8.0981e-03,  8.4909e-03,  8.5176e-03,  ...,  9.0476e-03,\n",
      "            8.9731e-03,  8.1686e-03],\n",
      "          ...,\n",
      "          [ 8.2039e-03,  8.6904e-03,  8.8556e-03,  ...,  8.6511e-03,\n",
      "            8.6360e-03,  7.7912e-03],\n",
      "          [ 8.1213e-03,  8.7617e-03,  8.8698e-03,  ...,  8.5112e-03,\n",
      "            8.6777e-03,  8.0075e-03],\n",
      "          [ 7.8699e-03,  8.4702e-03,  8.3722e-03,  ...,  8.5278e-03,\n",
      "            8.4008e-03,  7.9559e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.9281e-03, -2.9031e-03, -2.8851e-03,  ..., -2.8238e-03,\n",
      "           -2.8710e-03, -3.0553e-03],\n",
      "          [-2.9376e-03, -2.9780e-03, -3.0759e-03,  ..., -2.8082e-03,\n",
      "           -3.0392e-03, -3.0496e-03],\n",
      "          [-2.9998e-03, -3.0691e-03, -3.1471e-03,  ..., -3.1528e-03,\n",
      "           -3.3576e-03, -3.2113e-03],\n",
      "          ...,\n",
      "          [-2.8628e-03, -2.9749e-03, -2.8403e-03,  ..., -3.1348e-03,\n",
      "           -3.2132e-03, -2.9378e-03],\n",
      "          [-2.8088e-03, -2.9535e-03, -2.9733e-03,  ..., -2.6954e-03,\n",
      "           -3.0130e-03, -3.0311e-03],\n",
      "          [-3.2998e-03, -3.3933e-03, -3.3513e-03,  ..., -3.2109e-03,\n",
      "           -3.2004e-03, -3.1471e-03]],\n",
      "\n",
      "         [[ 1.2280e-03,  1.2384e-03,  1.0841e-03,  ...,  1.0714e-03,\n",
      "            9.9465e-04,  9.0126e-04],\n",
      "          [ 8.6239e-04,  9.1065e-04,  9.4484e-04,  ...,  9.0691e-04,\n",
      "            8.5074e-04,  1.0297e-03],\n",
      "          [ 9.4222e-04,  8.5387e-04,  8.8493e-04,  ...,  7.7193e-04,\n",
      "            7.7860e-04,  9.3313e-04],\n",
      "          ...,\n",
      "          [ 8.3555e-04,  7.4433e-04,  7.4760e-04,  ...,  6.7245e-04,\n",
      "            6.5689e-04,  7.5362e-04],\n",
      "          [ 9.2685e-04,  6.4961e-04,  6.2124e-04,  ...,  9.2064e-04,\n",
      "            7.6964e-04,  8.0146e-04],\n",
      "          [ 1.3138e-03,  1.2290e-03,  1.0426e-03,  ...,  1.1885e-03,\n",
      "            1.2668e-03,  1.1912e-03]],\n",
      "\n",
      "         [[ 1.9560e-03,  2.4097e-03,  2.3905e-03,  ...,  2.2673e-03,\n",
      "            2.2101e-03,  2.2485e-03],\n",
      "          [ 1.9117e-03,  1.7956e-03,  1.8970e-03,  ...,  1.8977e-03,\n",
      "            1.8065e-03,  1.7968e-03],\n",
      "          [ 1.7936e-03,  1.8756e-03,  1.9576e-03,  ...,  1.7798e-03,\n",
      "            1.6675e-03,  1.8679e-03],\n",
      "          ...,\n",
      "          [ 1.9489e-03,  1.9389e-03,  1.9246e-03,  ...,  2.0389e-03,\n",
      "            1.9972e-03,  1.7555e-03],\n",
      "          [ 2.0242e-03,  1.9350e-03,  1.9304e-03,  ...,  1.9420e-03,\n",
      "            1.6964e-03,  2.0842e-03],\n",
      "          [ 2.1424e-03,  1.8558e-03,  1.9271e-03,  ...,  2.1539e-03,\n",
      "            2.0570e-03,  1.7790e-03]]]], grad_fn=<MulBackward0>)]\n",
      "(Pdb) output[0].shape\n",
      "torch.Size([2, 32, 57, 77])\n",
      "(Pdb) output.reshape(output.size(0), -1)\n",
      "*** AttributeError: 'list' object has no attribute 'reshape'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Pdb) output[0].reshape(output[0].size(0), -1)\n",
      "tensor([[-1.1149e-05, -3.7266e-04, -4.0491e-04,  ...,  1.7492e-03,\n",
      "          1.6020e-03,  1.7844e-03],\n",
      "        [-2.2856e-05, -3.4150e-04, -4.4304e-04,  ...,  2.1539e-03,\n",
      "          2.0570e-03,  1.7790e-03]], grad_fn=<AsStridedBackward>)\n",
      "(Pdb) output[0].reshape(output[0].size(0), -1).shape\n",
      "torch.Size([2, 140448])\n"
     ]
    }
   ],
   "source": [
    "for i, (x, label) in enumerate(train_dataloader):\n",
    "    x, label = x.to(device).float(), label.to(device)\n",
    "    model(x).shape\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(x, y):\n",
    "    total = 0\n",
    "    for v in range(len(x)):\n",
    "        if x[v] and y[v]:\n",
    "            total += 1\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c6nDA4eylV6N"
   },
   "outputs": [],
   "source": [
    "def train(num_epochs, model, save):\n",
    "    EPOCH_TRAIN_LOSSES = []\n",
    "    EPOCH_VAL_LOSSES = []\n",
    "    EPOCH_TRAIN_ACC = []\n",
    "    EPOCH_VAL_ACC = []\n",
    "    train_a_acc = []\n",
    "    val_a_acc = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        running_train_loss = 0.0\n",
    "        running_val_loss = 0.0\n",
    "        train_correct = 0.0\n",
    "        val_correct = 0.0\n",
    "        train_total = 0.0\n",
    "        val_total = 0.0\n",
    "               \n",
    "        t_correct_predictions_0_5 = 0.0\n",
    "        t_correct_predictions_1 = 0.0\n",
    "        t_correct_predictions_2 = 0.0\n",
    "        t_correct_predictions_5 = 0.0\n",
    "        t_correct_predictions_g_5 = 0.0\n",
    "        \n",
    "        v_correct_predictions_0_5 = 0.0\n",
    "        v_correct_predictions_1 = 0.0\n",
    "        v_correct_predictions_2 = 0.0\n",
    "        v_correct_predictions_5 = 0.0\n",
    "        v_correct_predictions_g_5 = 0.0\n",
    "\n",
    "        model.train()\n",
    "        for i, (x, label) in enumerate(train_dataloader):\n",
    "            x, label = x.to(device), label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y = model(x.float()).double().squeeze(1)\n",
    "            \n",
    "            loss = criterion(y, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_train_loss += loss.item()\n",
    "            predicted_angles = y[0]\n",
    "            label_angles = label[0]\n",
    "            train_correct += torch.sum(torch.abs(predicted_angles - label_angles) < 0.1).item()\n",
    "            t_correct_predictions_0_5 += torch.sum(torch.abs(predicted_angles - label_angles) <= 0.5).item()\n",
    "            t_correct_predictions_1 += combine(torch.abs(predicted_angles - label_angles) > 0.5,  torch.abs(predicted_angles - label_angles) <= 1)\n",
    "            t_correct_predictions_2 += combine(torch.abs(predicted_angles - label_angles) > 1,  torch.abs(predicted_angles - label_angles) <= 2)\n",
    "            t_correct_predictions_5 += combine(torch.abs(predicted_angles - label_angles) > 2,  torch.abs(predicted_angles - label_angles) <= 5)\n",
    "            v_correct_predictions_g_5 += torch.sum(torch.abs(predicted_angles - label_angles) > 5).item()\n",
    "            train_total += label_angles.size(0)\n",
    "            \n",
    "            del x\n",
    "            del label\n",
    "                    \n",
    "        train_a_a = [t_correct_predictions_0_5, t_correct_predictions_1, t_correct_predictions_2, t_correct_predictions_5, t_correct_predictions_g_5]\n",
    "        train_a_acc.append(train_a_a)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (x, label) in enumerate(val_dataloader):\n",
    "                x, label = x.to(device), label.to(device)\n",
    "                y = model(x.float()).double().squeeze(1)\n",
    "                \n",
    "                loss = criterion(y, label)\n",
    "\n",
    "                running_val_loss += loss.item()\n",
    "                predicted_angles = y[0]\n",
    "                label_angles = label[0]\n",
    "                val_correct += torch.sum(torch.abs(predicted_angles - label_angles) < 0.1).item()\n",
    "                v_correct_predictions_0_5 += torch.sum(torch.abs(predicted_angles - label_angles) <= 0.5).item()\n",
    "                v_correct_predictions_1 += combine(torch.abs(predicted_angles - label_angles) > 0.5,  torch.abs(predicted_angles - label_angles) <= 1)\n",
    "                v_correct_predictions_2 += combine(torch.abs(predicted_angles - label_angles) > 1,  torch.abs(predicted_angles - label_angles) <= 2)\n",
    "                v_correct_predictions_5 += combine(torch.abs(predicted_angles - label_angles) > 2,  torch.abs(predicted_angles - label_angles) <= 5)\n",
    "                v_correct_predictions_g_5 += torch.sum(torch.abs(predicted_angles - label_angles) > 5).item()\n",
    "                val_total += label_angles.size(0)\n",
    "                \n",
    "            val_a_a = [v_correct_predictions_0_5, v_correct_predictions_1, v_correct_predictions_2, v_correct_predictions_5, v_correct_predictions_g_5]\n",
    "            val_a_acc.append(val_a_a)\n",
    "                \n",
    "        train_acc = (train_correct / (train_total))*100\n",
    "        val_acc = (val_correct / (val_total))*100\n",
    "        tloss = running_train_loss / len(train_dataloader)\n",
    "        vloss = running_val_loss/ len(val_dataloader)\n",
    "\n",
    "        print(\"EPOCH\", epoch, \"\\t\\tTook\", int(time.time() - start), \"s\")\n",
    "        print(\"Train Acc:\", train_acc, \"\\tVal Acc:\", val_acc)\n",
    "        print(\"Avg Train Loss:\", tloss, \"\\tAvg Val Loss:\", vloss)\n",
    "\n",
    "        scheduler.step(vloss)\n",
    "\n",
    "        EPOCH_TRAIN_LOSSES.append(tloss)\n",
    "        EPOCH_VAL_LOSSES.append(vloss)\n",
    "        EPOCH_TRAIN_ACC.append(train_acc)\n",
    "        EPOCH_VAL_ACC.append(val_acc)\n",
    "\n",
    "        if save:\n",
    "            torch.save(model.state_dict(), './model_' + str(epoch + 1) + '_' + str(val_acc) + '.pt')\n",
    "            \n",
    "    return EPOCH_TRAIN_LOSSES, EPOCH_VAL_LOSSES, EPOCH_TRAIN_ACC, EPOCH_VAL_ACC, val_a_acc, train_a_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Took 33 s\n",
      "Train Acc: 4.761904761904762 Val Acc: 11.11111111111111\n",
      "Avg Train Loss: 7.9671636295844985 Avg Val Loss: 117.09070854836399\n",
      "Epoch 1  Took 34 s\n",
      "Train Acc: 4.761904761904762 Val Acc: 0.0\n",
      "Avg Train Loss: 13.068651519256646 Avg Val Loss: 85.30155254776155\n",
      "Epoch 2  Took 30 s\n",
      "Train Acc: 0.0 Val Acc: 11.11111111111111\n",
      "Avg Train Loss: 5.258509106190407 Avg Val Loss: 76.39166465605946\n",
      "Epoch 3  Took 26 s\n",
      "Train Acc: 0.0 Val Acc: 11.11111111111111\n",
      "Avg Train Loss: 2.605073006538884 Avg Val Loss: 49.74188311403517\n",
      "Epoch 4  Took 28 s\n",
      "Train Acc: 4.761904761904762 Val Acc: 11.11111111111111\n",
      "Avg Train Loss: 3.719161066516466 Avg Val Loss: 31.232043613123235\n",
      "Epoch 5  Took 27 s\n",
      "Train Acc: 14.285714285714285 Val Acc: 11.11111111111111\n",
      "Avg Train Loss: 2.4621133901009165 Avg Val Loss: 20.95434440675994\n",
      "Epoch 6  Took 27 s\n",
      "Train Acc: 4.761904761904762 Val Acc: 22.22222222222222\n",
      "Avg Train Loss: 3.2779420196257023 Avg Val Loss: 9.907844510020544\n",
      "Epoch 7  Took 28 s\n",
      "Train Acc: 19.047619047619047 Val Acc: 22.22222222222222\n",
      "Avg Train Loss: 1.8925968037108298 Avg Val Loss: 3.086413749275314\n",
      "Epoch 8  Took 29 s\n",
      "Train Acc: 14.285714285714285 Val Acc: 22.22222222222222\n",
      "Avg Train Loss: 2.7380741526240584 Avg Val Loss: 1.7169819413686913\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-585765720d57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mt_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "n_epochs = 9\n",
    "EPOCH_TRAIN_LOSSES, EPOCH_VAL_LOSSES, EPOCH_TRAIN_ACC, EPOCH_VAL_ACC, val_a_acc, train_a_acc = train(n_epochs, model, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.arange(n_epochs)\n",
    "\n",
    "rangeVi = 2\n",
    "rangeVf = 9\n",
    "plt.plot(x[rangeVi:rangeVf], t_l[rangeVi:rangeVf], label=\"Train\")\n",
    "plt.plot(x[rangeVi:rangeVf], v_l[rangeVi:rangeVf], label=\"Validation\")\n",
    "plt.xlabel(\"Number of Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Epochs vs Loss\")\n",
    "plt.legend()\n",
    "plt.savefig('RNNLossFinal_ResNet_34.png')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x, t_a, label=\"Train\")\n",
    "plt.plot(x, v_a, label=\"Validation\")\n",
    "plt.xlabel(\"Number of Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Epochs vs Accuracy\")\n",
    "plt.legend()\n",
    "plt.savefig('RNNAccuracyFinal.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "steering.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
